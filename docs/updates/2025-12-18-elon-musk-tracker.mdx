---
title: "Walkthrough - Creating an Information Hub Agent Network"
summary: "Learn how to build an information hub network that collects data from multiple sources and exposes it via MCP for querying by Claude or other AI assistants."
topicTitle: "Updates"
topicSlug: "updates"
---

# Walkthrough: Creating an Information Hub Agent Network

This walkthrough teaches you how to build an **information hub** - a network that continuously collects data from external sources and makes it queryable by AI assistants via MCP. We'll use a news tracker as our example, but the same pattern applies to any data aggregation use case.

By the end, you'll understand how to:
- Create networks that serve as persistent data stores
- Build agents that continuously collect and organize information
- Expose your data hub to AI assistants like Claude via MCP

<Banner type="important">
**Important:** Please stop any running networks before starting this walkthrough.
</Banner>

## What You'll Learn

- Setting up a network with the **feed mod** for immutable post storage
- Creating a Python-based **WorkerAgent** for continuous data collection
- Fetching data from multiple RSS/API sources
- Connecting Claude Desktop or other MCP clients to query your information hub

## Architecture Overview

The information hub pattern consists of three key components:

```
┌─────────────────────────────────────────────────────────────┐
│                    OpenAgents Network                        │
│                                                              │
│  ┌──────────────┐     ┌──────────────┐                      │
│  │    Data      │────>│   Feed Mod   │<────── MCP ─────────┐│
│  │  Collector   │     │   (Storage)  │                     ││
│  │    Agent     │     └──────────────┘                     ││
│  └──────────────┘            │                             ││
│         │                    │                             ││
│         ▼                    ▼                             ││
│  ┌──────────────┐     ┌──────────────┐                     ││
│  │ External     │     │  Searchable  │                     ││
│  │ Data Sources │     │    Posts     │                     ││
│  │ (RSS, APIs)  │     │  + Tags      │                     ││
│  └──────────────┘     └──────────────┘                     ││
└─────────────────────────────────────────────────────────────┘│
                                                               │
┌──────────────────────────────────────────────────────────────┘
│
│   ┌──────────────────┐
└──>│  Claude Desktop  │ "What's the latest news?"
    │    (via MCP)     │
    └──────────────────┘
```

## Core Components

| Component | Type | Description |
|-----------|------|-------------|
| **Network** | Feed Mod | Immutable post storage with full-text search |
| **Collector Agent** | Python (WorkerAgent) | Fetches data on a schedule |
| **MCP Endpoint** | HTTP Transport | External access for AI assistants |

## Example: News Tracker Data Sources

In our example, we collect news from multiple free sources:

| Source | What It Captures |
|--------|------------------|
| Google News RSS | Mainstream news articles |
| Reddit RSS | Community discussions from relevant subreddits |
| Hacker News | Tech community discussions |

## Prerequisites

- OpenAgents installed (`pip install -e .`)
- Python 3.10 or higher

No additional API keys required - all data sources in this example are free!

## Step 1: Run the Example

Let's start by running the complete example, then we'll break down how each part works.

### Quick Start (One Command)

```bash
cd demos/06_elon_musk_tracker
./run.sh
```

### Manual Start (Two Terminals)

**Terminal 1: Start the Network**

```bash
cd demos/06_elon_musk_tracker
openagents network start network.yaml
```

Expected output:

```
INFO:openagents:Starting network: ElonMuskTracker
INFO:openagents:HTTP transport listening on 0.0.0.0:8700
INFO:openagents:gRPC transport listening on 0.0.0.0:8600
INFO:openagents:MCP endpoint available at http://localhost:8700/mcp
INFO:openagents:Loaded mod: openagents.mods.workspace.feed
```

**Terminal 2: Start the Collector Agent**

```bash
cd demos/06_elon_musk_tracker
python agents/elon_fan_agent.py
```

Expected output:

```
[NewsCollector] Connected! Starting news collection loop (interval: 300s)
[NewsCollector] Fetching news at 2025-12-18 10:30:00...
[NewsCollector] Found 12 new items to post.
[NewsCollector] Posted: Latest news headline...
```

### Command Line Options

```bash
python agents/elon_fan_agent.py --interval 60   # Fetch every 60 seconds
python agents/elon_fan_agent.py --port 8700     # Custom network port
python agents/elon_fan_agent.py --host myhost   # Custom network host
```

## Step 2: Connect AI Assistants via MCP

Once your information hub is running, you can connect AI assistants to query it via MCP.

### Configure Claude Desktop

**macOS:** Edit `~/Library/Application Support/Claude/claude_desktop_config.json`

**Linux:** Edit `~/.config/claude/claude_desktop_config.json`

**Windows:** Edit `%APPDATA%\Claude\claude_desktop_config.json`

<Banner type="info">
**Note:** Claude Desktop's "Add custom connector" UI requires HTTPS URLs. For local development with HTTP, use this config file method with `npx` to bridge the connection.
</Banner>

```json
{
  "mcpServers": {
    "my-info-hub": {
      "command": "npx",
      "args": [
        "mcp-remote",
        "http://localhost:8700/mcp"
      ]
    }
  }
}
```

Restart Claude Desktop after saving the configuration.

### Query Your Information Hub

Once connected, Claude can query your data using natural language:

| Query | What Claude Does |
|-------|------------------|
| "What's the latest news?" | Calls `get_recent_feed_posts` then summarizes |
| "Search for technology news" | Calls `search_feed_posts` with query |
| "Show me posts tagged 'important'" | Calls `list_feed_posts` with tag filter |
| "Get the latest 5 posts" | Calls `list_feed_posts` with limit 5 |

## MCP Tools Reference

The feed mod exposes these tools via MCP:

| Tool | Parameters | Description |
|------|------------|-------------|
| `list_feed_posts` | `limit`, `offset`, `tags`, `sort_by` | List posts with filters |
| `search_feed_posts` | `query`, `limit` | Full-text search |
| `get_recent_feed_posts` | `since_timestamp`, `limit` | Get posts since timestamp |
| `get_feed_post` | `post_id` | Get specific post |
| `create_feed_post` | `title`, `content`, `tags` | Create a new post |

## Step 3: Understanding the Network Configuration

The network configuration defines how your information hub operates.

### network.yaml Explained

```yaml
network:
  name: "MyInfoHub"
  mode: "centralized"
  node_id: "info-hub-1"

  transports:
    - type: "http"
      config:
        port: 8700
        serve_studio: true   # Web UI at http://localhost:8700
        serve_mcp: true      # MCP at http://localhost:8700/mcp
    - type: "grpc"
      config:
        port: 8600

  mods:
    - name: "openagents.mods.workspace.feed"
      enabled: true
      config:
        max_title_length: 200
        max_content_length: 50000
        enable_search: true
        categories:          # Define your own categories
          - "news"
          - "important"
          - "updates"
```

**Key configuration points:**
- `serve_mcp: true` - Enables the MCP endpoint for AI assistant access
- `enable_search: true` - Enables full-text search on posts
- `categories` - Define tags that can be used to organize posts

## Step 4: Building the Collector Agent

The collector agent is the heart of your information hub. It continuously fetches data and posts it to the feed.

### The WorkerAgent Pattern

```python
from openagents.agents.worker_agent import WorkerAgent

class DataCollectorAgent(WorkerAgent):
    default_agent_id = "my-collector"

    def __init__(self, fetch_interval: int = 300, **kwargs):
        super().__init__(**kwargs)
        self.fetch_interval = fetch_interval
        self.posted_hashes = set()  # Deduplication

    async def on_startup(self):
        self._collection_task = asyncio.create_task(self._collection_loop())

    async def _collection_loop(self):
        while True:
            await self._fetch_and_post_data()
            await asyncio.sleep(self.fetch_interval)
```

**Key patterns:**
- Extend `WorkerAgent` for background processing
- Use `on_startup()` to begin the collection loop
- Track posted items to avoid duplicates

### Auto-Tagging Content

Automatically categorize posts based on content:

```python
def get_tags(content: str) -> List[str]:
    tags = ["news"]
    content_lower = content.lower()

    if "technology" in content_lower: tags.append("tech")
    if "finance" in content_lower: tags.append("finance")
    if "breaking" in content_lower: tags.append("important")

    return tags
```

## Step 5: Adding Custom Data Sources

You can extend the collector to fetch from any data source.

### RSS Feed Example

```python
def fetch_rss(url: str, count: int = 10) -> List[Dict]:
    """Fetch from any RSS feed."""
    response = requests.get(url, timeout=15)
    root = ET.fromstring(response.content)

    items = []
    for item in root.findall(".//item")[:count]:
        items.append({
            "title": item.findtext("title", ""),
            "link": item.findtext("link", ""),
            "description": item.findtext("description", ""),
            "published_date": item.findtext("pubDate", ""),
            "feed_source": "custom"
        })
    return items
```

### API Integration Example

```python
async def fetch_api_data(api_url: str) -> List[Dict]:
    """Fetch from a REST API."""
    async with aiohttp.ClientSession() as session:
        async with session.get(api_url) as response:
            data = await response.json()
            return [
                {"title": item["name"], "content": item["description"]}
                for item in data["results"]
            ]
```

## Optional: Adding Authentication

You can add authentication to protect your information hub:

```yaml
agent_groups:
  admin:
    description: "Administrator agents with full permissions"
    password_hash: "8c6976e5b5410415bde908bd4dee15dfb167a9c873fc4bb8a81f6f2ab448a918"
    metadata:
      permissions: ["all"]
```

Generate a password hash with:

```python
import hashlib
hashlib.sha256("your-password".encode()).hexdigest()
```

## Data Storage

Feed posts are stored locally:

```
./data/your-hub-name/
└── feed/
    ├── posts/
    │   ├── {post_id_1}.json
    │   ├── {post_id_2}.json
    │   └── ...
    └── metadata.json
```

## Troubleshooting

### Network Won't Start

```bash
# Check if port is in use
lsof -i :8700

# Kill existing process
kill -9 $(lsof -t -i:8700)
```

### Agent Can't Connect

```bash
# Verify network is running
curl http://localhost:8700/health
```

### No Data Being Collected

1. Check internet connectivity
2. Some data sources may be rate-limited
3. Try increasing fetch interval: `--interval 600`

### MCP Connection Failed

```bash
# Test MCP endpoint directly
curl -X POST http://localhost:8700/mcp \
  -H "Content-Type: application/json" \
  -d '{"jsonrpc": "2.0", "method": "tools/list", "id": 1}'
```

## Example Files Reference

The demo in `demos/06_elon_musk_tracker/` provides a complete working example:

| File | Purpose |
|------|---------|
| `network.yaml` | Network configuration with feed mod |
| `agents/elon_fan_agent.py` | Worker agent that collects news |
| `tools/rss_fetcher.py` | RSS/API fetching utilities |
| `run.sh` | One-command demo launcher |

## Use Cases for Information Hubs

The information hub pattern applies to many scenarios:

| Use Case | Data Sources | Example Queries |
|----------|--------------|-----------------|
| News Aggregator | RSS feeds, APIs | "What happened today?" |
| Research Hub | Academic APIs, arXiv | "Find papers on transformers" |
| Social Monitor | Twitter API, Reddit | "What's trending?" |
| Price Tracker | E-commerce APIs | "Best laptop deals this week" |
| Job Board | Job APIs, Career sites | "Remote Python jobs" |

## Related Documentation

- **[Tech News Stream](/tutorials/demo-tech-news-stream)** - Similar pattern with chatroom integration
- **[Research Team](/tutorials/demo-research-team)** - Router-based task delegation
